{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os, sys, subprocess, glob, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse.linalg import svds\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RND = 42\n",
        "random.seed(RND)\n",
        "np.random.seed(RND)"
      ],
      "metadata": {
        "id": "U7BzoYl79oCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "print(\"Step 1: Please upload your kaggle.json (Colab will prompt a file chooser)...\")\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "jo7FMNxo9n5x",
        "outputId": "2b38f714-2dff-478b-9190-f3f2676c6acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Please upload your kaggle.json (Colab will prompt a file chooser)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-89f1997c-b8eb-4009-8099-7ad3dd9f55a2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-89f1997c-b8eb-4009-8099-7ad3dd9f55a2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (5).json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
        "if \"kaggle.json\" in uploaded:\n",
        "    open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"wb\").write(uploaded[\"kaggle.json\"])\n",
        "    os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\n",
        "    print(\"kaggle.json saved to ~/.kaggle/kaggle.json\")\n",
        "else:\n",
        "    # if uploaded file has different name (Colab may rename), try to find first json\n",
        "    json_files = [f for f in uploaded.keys() if f.lower().endswith(\".json\")]\n",
        "    if json_files:\n",
        "        src = list(uploaded.keys())[0]\n",
        "        open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"wb\").write(uploaded[src])\n",
        "        os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\n",
        "        print(f\"{src} saved to ~/.kaggle/kaggle.json\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No kaggle.json uploaded. Please upload your Kaggle API token file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sock4JYl9n2a",
        "outputId": "75bd753d-d8ef-4f4e-e424-1afb36db8fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle (5).json saved to ~/.kaggle/kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"./data\", exist_ok=True)\n",
        "datasets_to_try = [\n",
        "    \"grouplens/movielens-100k\",          # preferred official source (may require accepting terms on Kaggle)\n",
        "    \"prajitdatta/movielens-100k-dataset\" # alternative mirror if available\n",
        "]\n",
        "\n",
        "downloaded = False\n",
        "for ds in datasets_to_try:\n",
        "    print(f\"\\nAttempting to download dataset: {ds} ...\")\n",
        "    ret = subprocess.run([\"kaggle\", \"datasets\", \"download\", \"-d\", ds, \"-p\", \"./data\"], capture_output=True, text=True)\n",
        "    print(ret.stdout)\n",
        "    if ret.returncode == 0:\n",
        "        print(\"Download command returned success.\")\n",
        "        downloaded = True\n",
        "        break\n",
        "    else:\n",
        "        print(\"Download failed or returned non-zero. stderr:\")\n",
        "        print(ret.stderr)\n",
        "\n",
        "if not downloaded:\n",
        "    raise RuntimeError(\n",
        "        \"Failed to download MovieLens from Kaggle. Common reasons:\\n\"\n",
        "        \"- You need to ACCEPT the dataset license on Kaggle (open the dataset page and click Accept).\\n\"\n",
        "        \"- Kaggle rate limits or network issues.\\n\\n\"\n",
        "        \"Open https://www.kaggle.com/datasets/grouplens/movielens-100k in your browser, accept the terms, then re-run this cell.\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePbVozDT9nyQ",
        "outputId": "a1fe75f3-554f-4cef-b275-ab50928ae4f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Attempting to download dataset: grouplens/movielens-100k ...\n",
            "403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/datasets/metadata/grouplens/movielens-100k\n",
            "\n",
            "Download failed or returned non-zero. stderr:\n",
            "\n",
            "\n",
            "Attempting to download dataset: prajitdatta/movielens-100k-dataset ...\n",
            "Dataset URL: https://www.kaggle.com/datasets/prajitdatta/movielens-100k-dataset\n",
            "License(s): CC0-1.0\n",
            "movielens-100k-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "\n",
            "Download command returned success.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_candidates = glob.glob(\"./data/*.zip\")\n",
        "if not zip_candidates:\n",
        "    raise FileNotFoundError(\"No .zip files found in ./data after download. Check the download step or dataset filename.\")\n",
        "# prefer file containing 'movielens' or 'ml-100k'\n",
        "zip_file = None\n",
        "for z in zip_candidates:\n",
        "    if \"movielens\" in os.path.basename(z).lower() or \"ml-100k\" in os.path.basename(z).lower():\n",
        "        zip_file = z\n",
        "        break\n",
        "if zip_file is None:\n",
        "    zip_file = zip_candidates[0]\n",
        "print(f\"Unzipping {zip_file} ...\")\n",
        "ret = subprocess.run([\"unzip\",\"-o\", zip_file, \"-d\", \"./data\"], capture_output=True, text=True)\n",
        "print(ret.stdout)\n",
        "if ret.returncode != 0:\n",
        "    print(\"unzip stderr:\", ret.stderr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YrojYSI9nu3",
        "outputId": "30d25d60-656f-414e-c635-252145ee5aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping ./data/movielens-100k-dataset.zip ...\n",
            "Archive:  ./data/movielens-100k-dataset.zip\n",
            "  inflating: ./data/ml-100k/README   \n",
            "  inflating: ./data/ml-100k/allbut.pl  \n",
            "  inflating: ./data/ml-100k/mku.sh   \n",
            "  inflating: ./data/ml-100k/u.data   \n",
            "  inflating: ./data/ml-100k/u.genre  \n",
            "  inflating: ./data/ml-100k/u.info   \n",
            "  inflating: ./data/ml-100k/u.item   \n",
            "  inflating: ./data/ml-100k/u.occupation  \n",
            "  inflating: ./data/ml-100k/u.user   \n",
            "  inflating: ./data/ml-100k/u1.base  \n",
            "  inflating: ./data/ml-100k/u1.test  \n",
            "  inflating: ./data/ml-100k/u2.base  \n",
            "  inflating: ./data/ml-100k/u2.test  \n",
            "  inflating: ./data/ml-100k/u3.base  \n",
            "  inflating: ./data/ml-100k/u3.test  \n",
            "  inflating: ./data/ml-100k/u4.base  \n",
            "  inflating: ./data/ml-100k/u4.test  \n",
            "  inflating: ./data/ml-100k/u5.base  \n",
            "  inflating: ./data/ml-100k/u5.test  \n",
            "  inflating: ./data/ml-100k/ua.base  \n",
            "  inflating: ./data/ml-100k/ua.test  \n",
            "  inflating: ./data/ml-100k/ub.base  \n",
            "  inflating: ./data/ml-100k/ub.test  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_ml100k_paths(base_dir=\"./data\"):\n",
        "    candidates = []\n",
        "    # typical locations\n",
        "    candidates.append(os.path.join(base_dir, \"ml-100k\", \"u.data\"))\n",
        "    candidates.append(os.path.join(base_dir, \"ml-100k\", \"u.item\"))\n",
        "    candidates.append(os.path.join(base_dir, \"u.data\"))\n",
        "    candidates.append(os.path.join(base_dir, \"u.item\"))\n",
        "    # also try nested\n",
        "    for root, dirs, files in os.walk(base_dir):\n",
        "        for f in files:\n",
        "            if f.lower() == \"u.data\":\n",
        "                candidates.append(os.path.join(root, f))\n",
        "            if f.lower() == \"u.item\":\n",
        "                candidates.append(os.path.join(root, f))\n",
        "    # pick first pair that exists\n",
        "    udata = None\n",
        "    uitem = None\n",
        "    for p in candidates:\n",
        "        if os.path.basename(p).lower() == \"u.data\" and os.path.isfile(p):\n",
        "            udata = p\n",
        "        if os.path.basename(p).lower() == \"u.item\" and os.path.isfile(p):\n",
        "            uitem = p\n",
        "    return udata, uitem\n",
        "\n",
        "u_data_path, u_item_path = find_ml100k_paths(\"./data\")\n",
        "if not u_data_path or not u_item_path:\n",
        "    raise FileNotFoundError(\"Could not locate u.data and u.item after unzipping. Check that the MovieLens 100K files exist under ./data or ./data/ml-100k.\")\n",
        "\n",
        "print(\"Found u.data at:\", u_data_path)\n",
        "print(\"Found u.item at:\", u_item_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiGJfKjT-B2W",
        "outputId": "ab535dc5-280a-41f5-9d49-fbcb3266d650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found u.data at: ./data/ml-100k/u.data\n",
            "Found u.item at: ./data/ml-100k/u.item\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLoading dataset into pandas...\")\n",
        "ratings = pd.read_csv(u_data_path, sep='\\t', names=['user_id','movie_id','rating','timestamp'], encoding='latin-1')\n",
        "movies = pd.read_csv(u_item_path, sep='|', header=None, encoding='latin-1', low_memory=False)\n",
        "# Keep first two columns: id and title (some mirrors have different columns)\n",
        "movies = movies.iloc[:, :2]\n",
        "movies.columns = ['movie_id', 'title']\n",
        "print(\"Ratings:\", ratings.shape, \"Movies:\", movies.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlTGXQem-By5",
        "outputId": "0312efe5-5e60-4680-a007-6cdc0b0635e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading dataset into pandas...\n",
            "Ratings: (100000, 4) Movies: (1682, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def leave_one_out(df, seed=RND):\n",
        "    users = df['user_id'].unique()\n",
        "    rng = np.random.RandomState(seed)\n",
        "    train_parts = []\n",
        "    test_parts = []\n",
        "    for u in users:\n",
        "        sub = df[df['user_id'] == u]\n",
        "        if len(sub) <= 1:\n",
        "            train_parts.append(sub)\n",
        "            continue\n",
        "        idx = rng.choice(sub.index, 1, replace=False)\n",
        "        test_parts.append(sub.loc[idx])\n",
        "        train_parts.append(sub.drop(idx))\n",
        "    train_df = pd.concat(train_parts).reset_index(drop=True)\n",
        "    test_df = pd.concat(test_parts).reset_index(drop=True)\n",
        "    return train_df, test_df\n",
        "\n",
        "train_df, test_df = leave_one_out(ratings)\n",
        "print(\"Train ratings:\", len(train_df), \"Test (held-out) ratings:\", len(test_df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIrppeT7-BwR",
        "outputId": "8052b808-3961-4f58-9c69-81fbd000c63f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train ratings: 99057 Test (held-out) ratings: 943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ui = train_df.pivot_table(index='user_id', columns='movie_id', values='rating')\n",
        "# ensure columns include all movies so shape consistent\n",
        "all_movie_ids = movies['movie_id'].unique().tolist()\n",
        "train_ui = train_ui.reindex(columns=all_movie_ids)\n",
        "print(\"Train user-item matrix shape:\", train_ui.shape)\n",
        "\n",
        "# helper mappings\n",
        "user_to_idx = {u: i for i, u in enumerate(train_ui.index)}\n",
        "idx_to_user = {i: u for u, i in user_to_idx.items()}\n",
        "movie_to_idx = {m: i for i, m in enumerate(train_ui.columns)}\n",
        "idx_to_movie = {i: m for m, i in movie_to_idx.items()}\n",
        "id2title = dict(zip(movies['movie_id'], movies['title']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7FNDZxZ-Bt2",
        "outputId": "8fe7dfb2-c755-466c-f1a5-67561b1bc9dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train user-item matrix shape: (943, 1682)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_user_sim(train_ui, normalize=True, fillna=0.0):\n",
        "    M = train_ui.copy()\n",
        "    if normalize:\n",
        "        M = M.subtract(M.mean(axis=1), axis=0)\n",
        "    mat = M.fillna(fillna).values\n",
        "    sim = cosine_similarity(mat)\n",
        "    np.fill_diagonal(sim, 0.0)\n",
        "    return sim\n",
        "\n",
        "print(\"Computing user-user similarity (cosine on demeaned ratings)...\")\n",
        "user_sim = compute_user_sim(train_ui, normalize=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLvI2sWH-Bq0",
        "outputId": "c04f45ee-3797-40fa-dea9-2e91acb476a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing user-user similarity (cosine on demeaned ratings)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_user_based(user_id, top_k=10, n_neighbors=20):\n",
        "    if user_id not in user_to_idx:\n",
        "        return []\n",
        "    uidx = user_to_idx[user_id]\n",
        "    sims = user_sim[uidx].copy()\n",
        "    sims[uidx] = 0.0\n",
        "    neigh_idx = np.argsort(sims)[::-1][:n_neighbors]\n",
        "    neigh_sims = sims[neigh_idx]\n",
        "    R = train_ui.values  # users x items (movie_id columns)\n",
        "    numer = (neigh_sims[:, None] * np.nan_to_num(R[neigh_idx, :], nan=0.0)).sum(axis=0)\n",
        "    denom = (neigh_sims[:, None] * (~np.isnan(R[neigh_idx, :])).astype(float)).sum(axis=0)\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        preds = numer / denom\n",
        "    preds[np.isnan(preds)] = -np.inf\n",
        "    seen_mask = ~np.isnan(train_ui.loc[user_id].values)\n",
        "    preds[seen_mask] = -np.inf\n",
        "    top_idx = np.argsort(preds)[::-1][:top_k]\n",
        "    recs = [(idx_to_movie[i], float(preds[i])) for i in top_idx if preds[i] != -np.inf]\n",
        "    return recs"
      ],
      "metadata": {
        "id": "43vUpeZh-BoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Computing item-item similarity (cosine)...\")\n",
        "item_mat = train_ui.fillna(0.0).values.T  # items x users\n",
        "item_sim = cosine_similarity(item_mat)\n",
        "np.fill_diagonal(item_sim, 0.0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okrWfrbS-Bld",
        "outputId": "89055ddd-ab26-47f3-97d9-97194a6ea6a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing item-item similarity (cosine)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Colab-ready pipeline: upload kaggle.json -> download MovieLens100k -> run recommenders + eval\n",
        "# Run this whole cell in Google Colab.\n",
        "\n",
        "# 0) Imports\n",
        "import os, sys, subprocess, glob, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse.linalg import svds\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RND = 42\n",
        "random.seed(RND)\n",
        "np.random.seed(RND)\n",
        "\n",
        "# 1) Upload kaggle.json (Colab UI)\n",
        "from google.colab import files\n",
        "print(\"Step 1: Please upload your kaggle.json (Colab will prompt a file chooser)...\")\n",
        "uploaded = files.upload()  # choose kaggle.json\n",
        "\n",
        "# 2) Setup ~/.kaggle\n",
        "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
        "if \"kaggle.json\" in uploaded:\n",
        "    open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"wb\").write(uploaded[\"kaggle.json\"])\n",
        "    os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\n",
        "    print(\"kaggle.json saved to ~/.kaggle/kaggle.json\")\n",
        "else:\n",
        "    # if uploaded file has different name (Colab may rename), try to find first json\n",
        "    json_files = [f for f in uploaded.keys() if f.lower().endswith(\".json\")]\n",
        "    if json_files:\n",
        "        src = list(uploaded.keys())[0]\n",
        "        open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"wb\").write(uploaded[src])\n",
        "        os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\n",
        "        print(f\"{src} saved to ~/.kaggle/kaggle.json\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No kaggle.json uploaded. Please upload your Kaggle API token file.\")\n",
        "\n",
        "# 3) Download MovieLens 100K (try primary dataset id, fallback to alternative)\n",
        "os.makedirs(\"./data\", exist_ok=True)\n",
        "datasets_to_try = [\n",
        "    \"grouplens/movielens-100k\",          # preferred official source (may require accepting terms on Kaggle)\n",
        "    \"prajitdatta/movielens-100k-dataset\" # alternative mirror if available\n",
        "]\n",
        "\n",
        "downloaded = False\n",
        "for ds in datasets_to_try:\n",
        "    print(f\"\\nAttempting to download dataset: {ds} ...\")\n",
        "    ret = subprocess.run([\"kaggle\", \"datasets\", \"download\", \"-d\", ds, \"-p\", \"./data\"], capture_output=True, text=True)\n",
        "    print(ret.stdout)\n",
        "    if ret.returncode == 0:\n",
        "        print(\"Download command returned success.\")\n",
        "        downloaded = True\n",
        "        break\n",
        "    else:\n",
        "        print(\"Download failed or returned non-zero. stderr:\")\n",
        "        print(ret.stderr)\n",
        "\n",
        "if not downloaded:\n",
        "    raise RuntimeError(\n",
        "        \"Failed to download MovieLens from Kaggle. Common reasons:\\n\"\n",
        "        \"- You need to ACCEPT the dataset license on Kaggle (open the dataset page and click Accept).\\n\"\n",
        "        \"- Kaggle rate limits or network issues.\\n\\n\"\n",
        "        \"Open https://www.kaggle.com/datasets/grouplens/movielens-100k in your browser, accept the terms, then re-run this cell.\"\n",
        "    )\n",
        "\n",
        "# 4) Unzip any movielens zip present in ./data\n",
        "zip_candidates = glob.glob(\"./data/*.zip\")\n",
        "if not zip_candidates:\n",
        "    raise FileNotFoundError(\"No .zip files found in ./data after download. Check the download step or dataset filename.\")\n",
        "# prefer file containing 'movielens' or 'ml-100k'\n",
        "zip_file = None\n",
        "for z in zip_candidates:\n",
        "    if \"movielens\" in os.path.basename(z).lower() or \"ml-100k\" in os.path.basename(z).lower():\n",
        "        zip_file = z\n",
        "        break\n",
        "if zip_file is None:\n",
        "    zip_file = zip_candidates[0]\n",
        "print(f\"Unzipping {zip_file} ...\")\n",
        "ret = subprocess.run([\"unzip\",\"-o\", zip_file, \"-d\", \"./data\"], capture_output=True, text=True)\n",
        "print(ret.stdout)\n",
        "if ret.returncode != 0:\n",
        "    print(\"unzip stderr:\", ret.stderr)\n",
        "\n",
        "# 5) Locate u.data and u.item (support various extraction structures)\n",
        "def find_ml100k_paths(base_dir=\"./data\"):\n",
        "    candidates = []\n",
        "    # typical locations\n",
        "    candidates.append(os.path.join(base_dir, \"ml-100k\", \"u.data\"))\n",
        "    candidates.append(os.path.join(base_dir, \"ml-100k\", \"u.item\"))\n",
        "    candidates.append(os.path.join(base_dir, \"u.data\"))\n",
        "    candidates.append(os.path.join(base_dir, \"u.item\"))\n",
        "    # also try nested\n",
        "    for root, dirs, files in os.walk(base_dir):\n",
        "        for f in files:\n",
        "            if f.lower() == \"u.data\":\n",
        "                candidates.append(os.path.join(root, f))\n",
        "            if f.lower() == \"u.item\":\n",
        "                candidates.append(os.path.join(root, f))\n",
        "    # pick first pair that exists\n",
        "    udata = None\n",
        "    uitem = None\n",
        "    for p in candidates:\n",
        "        if os.path.basename(p).lower() == \"u.data\" and os.path.isfile(p):\n",
        "            udata = p\n",
        "        if os.path.basename(p).lower() == \"u.item\" and os.path.isfile(p):\n",
        "            uitem = p\n",
        "    return udata, uitem\n",
        "\n",
        "u_data_path, u_item_path = find_ml100k_paths(\"./data\")\n",
        "if not u_data_path or not u_item_path:\n",
        "    raise FileNotFoundError(\"Could not locate u.data and u.item after unzipping. Check that the MovieLens 100K files exist under ./data or ./data/ml-100k.\")\n",
        "\n",
        "print(\"Found u.data at:\", u_data_path)\n",
        "print(\"Found u.item at:\", u_item_path)\n",
        "\n",
        "# 6) Load the MovieLens files\n",
        "print(\"\\nLoading dataset into pandas...\")\n",
        "ratings = pd.read_csv(u_data_path, sep='\\t', names=['user_id','movie_id','rating','timestamp'], encoding='latin-1')\n",
        "movies = pd.read_csv(u_item_path, sep='|', header=None, encoding='latin-1', low_memory=False)\n",
        "# Keep first two columns: id and title (some mirrors have different columns)\n",
        "movies = movies.iloc[:, :2]\n",
        "movies.columns = ['movie_id', 'title']\n",
        "print(\"Ratings:\", ratings.shape, \"Movies:\", movies.shape)\n",
        "\n",
        "# 7) Build leave-one-out train/test (one holdout per user)\n",
        "def leave_one_out(df, seed=RND):\n",
        "    users = df['user_id'].unique()\n",
        "    rng = np.random.RandomState(seed)\n",
        "    train_parts = []\n",
        "    test_parts = []\n",
        "    for u in users:\n",
        "        sub = df[df['user_id'] == u]\n",
        "        if len(sub) <= 1:\n",
        "            train_parts.append(sub)\n",
        "            continue\n",
        "        idx = rng.choice(sub.index, 1, replace=False)\n",
        "        test_parts.append(sub.loc[idx])\n",
        "        train_parts.append(sub.drop(idx))\n",
        "    train_df = pd.concat(train_parts).reset_index(drop=True)\n",
        "    test_df = pd.concat(test_parts).reset_index(drop=True)\n",
        "    return train_df, test_df\n",
        "\n",
        "train_df, test_df = leave_one_out(ratings)\n",
        "print(\"Train ratings:\", len(train_df), \"Test (held-out) ratings:\", len(test_df))\n",
        "\n",
        "# 8) Build user-item matrix (use movie_id as columns to keep ids consistent)\n",
        "train_ui = train_df.pivot_table(index='user_id', columns='movie_id', values='rating')\n",
        "# ensure columns include all movies so shape consistent\n",
        "all_movie_ids = movies['movie_id'].unique().tolist()\n",
        "train_ui = train_ui.reindex(columns=all_movie_ids)\n",
        "print(\"Train user-item matrix shape:\", train_ui.shape)\n",
        "\n",
        "# helper mappings\n",
        "user_to_idx = {u: i for i, u in enumerate(train_ui.index)}\n",
        "idx_to_user = {i: u for u, i in user_to_idx.items()}\n",
        "movie_to_idx = {m: i for i, m in enumerate(train_ui.columns)}\n",
        "idx_to_movie = {i: m for m, i in movie_to_idx.items()}\n",
        "id2title = dict(zip(movies['movie_id'], movies['title']))\n",
        "\n",
        "# 9) User-based CF: compute user-user cosine similarity on demeaned ratings (better practice)\n",
        "def compute_user_sim(train_ui, normalize=True, fillna=0.0):\n",
        "    M = train_ui.copy()\n",
        "    if normalize:\n",
        "        M = M.subtract(M.mean(axis=1), axis=0)\n",
        "    mat = M.fillna(fillna).values\n",
        "    sim = cosine_similarity(mat)\n",
        "    np.fill_diagonal(sim, 0.0)\n",
        "    return sim\n",
        "\n",
        "print(\"Computing user-user similarity (cosine on demeaned ratings)...\")\n",
        "user_sim = compute_user_sim(train_ui, normalize=True)\n",
        "\n",
        "# user-based recommender returning movie_id + score\n",
        "def recommend_user_based(user_id, top_k=10, n_neighbors=20):\n",
        "    if user_id not in user_to_idx:\n",
        "        return []\n",
        "    uidx = user_to_idx[user_id]\n",
        "    sims = user_sim[uidx].copy()\n",
        "    sims[uidx] = 0.0\n",
        "    neigh_idx = np.argsort(sims)[::-1][:n_neighbors]\n",
        "    neigh_sims = sims[neigh_idx]\n",
        "    R = train_ui.values  # users x items (movie_id columns)\n",
        "    numer = (neigh_sims[:, None] * np.nan_to_num(R[neigh_idx, :], nan=0.0)).sum(axis=0)\n",
        "    denom = (neigh_sims[:, None] * (~np.isnan(R[neigh_idx, :])).astype(float)).sum(axis=0)\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        preds = numer / denom\n",
        "    preds[np.isnan(preds)] = -np.inf\n",
        "    seen_mask = ~np.isnan(train_ui.loc[user_id].values)\n",
        "    preds[seen_mask] = -np.inf\n",
        "    top_idx = np.argsort(preds)[::-1][:top_k]\n",
        "    recs = [(idx_to_movie[i], float(preds[i])) for i in top_idx if preds[i] != -np.inf]\n",
        "    return recs\n",
        "\n",
        "# 10) Item-based CF\n",
        "print(\"Computing item-item similarity (cosine)...\")\n",
        "item_mat = train_ui.fillna(0.0).values.T  # items x users\n",
        "item_sim = cosine_similarity(item_mat)\n",
        "np.fill_diagonal(item_sim, 0.0)\n",
        "\n",
        "def recommend_item_based(user_id, top_k=10, n_neighbors=20):\n",
        "    if user_id not in user_to_idx:\n",
        "        return []\n",
        "    user_row = train_ui.loc[user_id].values\n",
        "    seen = ~np.isnan(user_row)\n",
        "    unseen_idx = np.where(np.isnan(user_row))[0]\n",
        "    if unseen_idx.size == 0:\n",
        "        return []\n",
        "    preds = np.full(user_row.shape, -np.inf, dtype=float)\n",
        "    for j in unseen_idx:\n",
        "        sim_j = item_sim[j]\n",
        "        rated_idx = np.where(seen)[0]\n",
        "        if rated_idx.size == 0:\n",
        "            continue\n",
        "        # pick top neighbors among rated items\n",
        "        neigh = rated_idx[np.argsort(sim_j[rated_idx])[::-1][:n_neighbors]]\n",
        "        weights = sim_j[neigh]\n",
        "        ratings = user_row[neigh]\n",
        "        if weights.sum() == 0:\n",
        "            continue\n",
        "        preds[j] = np.dot(weights, ratings) / (weights.sum() + 1e-8)\n",
        "    top_idx = np.argsort(preds)[::-1][:top_k]\n",
        "    recs = [(idx_to_movie[i], float(preds[i])) for i in top_idx if preds[i] != -np.inf]\n",
        "    return recs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "id": "qNmTyLBD-Bi2",
        "outputId": "48ce2abd-1c1d-4119-d709-68dffb578d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Please upload your kaggle.json (Colab will prompt a file chooser)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c4856390-79f4-4f02-94bb-da323aed365f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c4856390-79f4-4f02-94bb-da323aed365f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (6).json\n",
            "kaggle (6).json saved to ~/.kaggle/kaggle.json\n",
            "\n",
            "Attempting to download dataset: grouplens/movielens-100k ...\n",
            "403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/datasets/metadata/grouplens/movielens-100k\n",
            "\n",
            "Download failed or returned non-zero. stderr:\n",
            "\n",
            "\n",
            "Attempting to download dataset: prajitdatta/movielens-100k-dataset ...\n",
            "Dataset URL: https://www.kaggle.com/datasets/prajitdatta/movielens-100k-dataset\n",
            "License(s): CC0-1.0\n",
            "movielens-100k-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "\n",
            "Download command returned success.\n",
            "Unzipping ./data/movielens-100k-dataset.zip ...\n",
            "Archive:  ./data/movielens-100k-dataset.zip\n",
            "  inflating: ./data/ml-100k/README   \n",
            "  inflating: ./data/ml-100k/allbut.pl  \n",
            "  inflating: ./data/ml-100k/mku.sh   \n",
            "  inflating: ./data/ml-100k/u.data   \n",
            "  inflating: ./data/ml-100k/u.genre  \n",
            "  inflating: ./data/ml-100k/u.info   \n",
            "  inflating: ./data/ml-100k/u.item   \n",
            "  inflating: ./data/ml-100k/u.occupation  \n",
            "  inflating: ./data/ml-100k/u.user   \n",
            "  inflating: ./data/ml-100k/u1.base  \n",
            "  inflating: ./data/ml-100k/u1.test  \n",
            "  inflating: ./data/ml-100k/u2.base  \n",
            "  inflating: ./data/ml-100k/u2.test  \n",
            "  inflating: ./data/ml-100k/u3.base  \n",
            "  inflating: ./data/ml-100k/u3.test  \n",
            "  inflating: ./data/ml-100k/u4.base  \n",
            "  inflating: ./data/ml-100k/u4.test  \n",
            "  inflating: ./data/ml-100k/u5.base  \n",
            "  inflating: ./data/ml-100k/u5.test  \n",
            "  inflating: ./data/ml-100k/ua.base  \n",
            "  inflating: ./data/ml-100k/ua.test  \n",
            "  inflating: ./data/ml-100k/ub.base  \n",
            "  inflating: ./data/ml-100k/ub.test  \n",
            "\n",
            "Found u.data at: ./data/ml-100k/u.data\n",
            "Found u.item at: ./data/ml-100k/u.item\n",
            "\n",
            "Loading dataset into pandas...\n",
            "Ratings: (100000, 4) Movies: (1682, 2)\n",
            "Train ratings: 99057 Test (held-out) ratings: 943\n",
            "Train user-item matrix shape: (943, 1682)\n",
            "Computing user-user similarity (cosine on demeaned ratings)...\n",
            "Computing item-item similarity (cosine)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def svd_predict(train_ui, k=25):\n",
        "    R = train_ui.copy()\n",
        "    user_means = R.mean(axis=1)\n",
        "    global_mean = R.stack().mean()\n",
        "    R_filled = R.copy()\n",
        "    for uid in R.index:\n",
        "        user_mean = user_means.loc[uid]\n",
        "        fill_val = user_mean if not np.isnan(user_mean) else global_mean\n",
        "        R_filled.loc[uid] = R_filled.loc[uid].fillna(fill_val)\n",
        "    M = R_filled.values\n",
        "    # demean columns for svds stability\n",
        "    col_mean = np.mean(M, axis=0)\n",
        "    M_centered = M - col_mean\n",
        "    k = min(k, min(M_centered.shape)-1)\n",
        "    if k < 2:\n",
        "        k = 2\n",
        "    U, s, Vt = svds(M_centered, k=k)\n",
        "    S = np.diag(s)\n",
        "    M_hat = np.dot(np.dot(U, S), Vt) + col_mean\n",
        "    pred_df = pd.DataFrame(M_hat, index=train_ui.index, columns=train_ui.columns)\n",
        "    return pred_df\n",
        "\n",
        "print(\"Computing SVD predictions (this may take a few seconds)...\")\n",
        "pred_svd = svd_predict(train_ui, k=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbA6P406-BgX",
        "outputId": "d2fe5e7f-bf8b-4f6f-a113-7af4e9b7d6f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing SVD predictions (this may take a few seconds)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def recommend_svd(user_id, top_k=10):\n",
        "    if user_id not in pred_svd.index:\n",
        "        return []\n",
        "    preds = pred_svd.loc[user_id].values.copy()\n",
        "    seen_mask = ~np.isnan(train_ui.loc[user_id].values)\n",
        "    preds[seen_mask] = -np.inf\n",
        "    top_idx = np.argsort(preds)[::-1][:top_k]\n",
        "    recs = [(idx_to_movie[i], float(preds[i])) for i in top_idx if preds[i] != -np.inf]\n",
        "    return recs"
      ],
      "metadata": {
        "id": "C3bxsuX--Bdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pairs = list(zip(test_df['user_id'].values, test_df['movie_id'].values))\n",
        "def precision_at_k_recommender(test_pairs, recommender_fn, k=10, **kwargs):\n",
        "    hits = []\n",
        "    for uid, true_mid in test_pairs:\n",
        "        recs = recommender_fn(uid, top_k=k, **kwargs)\n",
        "        rec_ids = [mid for mid, score in recs]\n",
        "        hits.append(1.0 if true_mid in rec_ids else 0.0)\n",
        "    return np.mean(hits) if hits else 0.0\n",
        "\n",
        "K = 10\n",
        "print(f\"\\nEvaluating Precision@{K} on {len(test_pairs)} held-out ratings...\")\n",
        "prec_user = precision_at_k_recommender(test_pairs, recommend_user_based, k=K, n_neighbors=20)\n",
        "prec_item = precision_at_k_recommender(test_pairs, recommend_item_based, k=K, n_neighbors=20)\n",
        "prec_svd  = precision_at_k_recommender(test_pairs, recommend_svd, k=K)\n",
        "\n",
        "print(f\"Precision@{K}  -> UserCF: {prec_user:.4f} | ItemCF: {prec_item:.4f} | SVD: {prec_svd:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMMbDCD7-Bbg",
        "outputId": "c296e909-da10-4ce4-cc80-efc4ae147853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating Precision@10 on 943 held-out ratings...\n",
            "Precision@10  -> UserCF: 0.0095 | ItemCF: 0.0148 | SVD: 0.1251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "sample_users = random.sample(list(train_ui.index), k=5)\n",
        "rows = []\n",
        "for uid in sample_users:\n",
        "    for model_name, fn in [(\"user\", recommend_user_based), (\"item\", recommend_item_based), (\"svd\", recommend_svd)]:\n",
        "        recs = fn(uid, top_k=10, n_neighbors=20) if model_name != \"svd\" else fn(uid, top_k=10)\n",
        "        for rank, (mid, score) in enumerate(recs, start=1):\n",
        "            rows.append({\"user_id\": uid, \"model\": model_name, \"rank\": rank, \"movie_id\": mid, \"title\": id2title.get(mid, \"\"), \"score\": score})\n",
        "pd.DataFrame(rows).to_csv(\"outputs/example_recommendations.csv\", index=False)\n",
        "print(\"Saved example_recommendations.csv to ./outputs/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIxgk7Gg-BYz",
        "outputId": "3625ee47-46e4-4928-f478-54c6f401411e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved example_recommendations.csv to ./outputs/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "fig = plt.figure(figsize=(8,4))\n",
        "sns.histplot(train_ui.stack().values, bins=6)\n",
        "plt.title(\"Ratings distribution (train)\")\n",
        "plt.savefig(\"outputs/ratings_distribution.png\")\n",
        "plt.close(fig)"
      ],
      "metadata": {
        "id": "NSmaS8Gl-BV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig2 = plt.figure(figsize=(8,4))\n",
        "user_density = (~train_ui.isna()).mean(axis=1)\n",
        "sns.histplot(user_density.values, bins=30)\n",
        "plt.title(\"User density (fraction of movies rated)\")\n",
        "plt.savefig(\"outputs/user_density.png\")\n",
        "plt.close(fig2)\n",
        "print(\"Saved diagnostics images to ./outputs/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qumaqopg-454",
        "outputId": "22e53170-4a32-41a8-d86b-a0f68093cf6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved diagnostics images to ./outputs/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSample recommendations (titles) for a few users:\")\n",
        "for uid in sample_users:\n",
        "    print(f\"\\nUser {uid}:\")\n",
        "    ur = recommend_user_based(uid, top_k=8, n_neighbors=20)\n",
        "    print(\" User-based:\")\n",
        "    for mid, score in ur:\n",
        "        print(\"   \", id2title.get(mid, f\"id:{mid}\"), f\"(score={score:.3f})\")\n",
        "    it = recommend_item_based(uid, top_k=8, n_neighbors=20)\n",
        "    print(\" Item-based:\")\n",
        "    for mid, score in it:\n",
        "        print(\"   \", id2title.get(mid, f\"id:{mid}\"), f\"(score={score:.3f})\")\n",
        "    sv = recommend_svd(uid, top_k=8)\n",
        "    print(\" SVD-based:\")\n",
        "    for mid, score in sv:\n",
        "        print(\"   \", id2title.get(mid, f\"id:{mid}\"), f\"(score={score:.3f})\")\n",
        "\n",
        "print(\"\\nAll done  outputs are in the ./outputs folder. If Kaggle download failed earlier, accept the dataset license on Kaggle and re-run the cell.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lnrZHlR-42c",
        "outputId": "cbf929c5-6256-4ff4-d081-4920dbd351e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample recommendations (titles) for a few users:\n",
            "\n",
            "User 655:\n",
            " User-based:\n",
            "    Maya Lin: A Strong Clear Vision (1994) (score=5.000)\n",
            "    Top Hat (1935) (score=5.000)\n",
            "    Kansas City (1996) (score=5.000)\n",
            "    Drop Dead Fred (1991) (score=5.000)\n",
            "    Old Lady Who Walked in the Sea, The (Vieille qui marchait dans la mer, La) (1991) (score=5.000)\n",
            "    Billy Madison (1995) (score=5.000)\n",
            "    Romper Stomper (1992) (score=5.000)\n",
            "    Someone Else's America (1995) (score=5.000)\n",
            " Item-based:\n",
            "    East of Eden (1955) (score=3.603)\n",
            "    It's a Wonderful Life (1946) (score=3.587)\n",
            "    Wild Bunch, The (1969) (score=3.587)\n",
            "    Cat on a Hot Tin Roof (1958) (score=3.569)\n",
            "    Singin' in the Rain (1952) (score=3.561)\n",
            "    Thin Man, The (1934) (score=3.534)\n",
            "    Apocalypse Now (1979) (score=3.510)\n",
            "    Bringing Up Baby (1938) (score=3.507)\n",
            " SVD-based:\n",
            "    It's a Wonderful Life (1946) (score=3.665)\n",
            "    Some Like It Hot (1959) (score=3.635)\n",
            "    Sunset Blvd. (1950) (score=3.476)\n",
            "    Ben-Hur (1959) (score=3.420)\n",
            "    Good, The Bad and The Ugly, The (1966) (score=3.407)\n",
            "    Evil Dead II (1987) (score=3.398)\n",
            "    Apocalypse Now (1979) (score=3.352)\n",
            "    Beauty and the Beast (1991) (score=3.349)\n",
            "\n",
            "User 115:\n",
            " User-based:\n",
            "    Big Sleep, The (1946) (score=5.000)\n",
            "    Fresh (1994) (score=5.000)\n",
            "    Touch of Evil (1958) (score=5.000)\n",
            "    Sleeper (1973) (score=5.000)\n",
            "    Braindead (1992) (score=5.000)\n",
            "    Supercop (1992) (score=5.000)\n",
            "    Koyaanisqatsi (1983) (score=5.000)\n",
            "    Maya Lin: A Strong Clear Vision (1994) (score=5.000)\n",
            " Item-based:\n",
            "    Until the End of the World (Bis ans Ende der Welt) (1991) (score=4.853)\n",
            "    Delicatessen (1991) (score=4.800)\n",
            "    Beat the Devil (1954) (score=4.749)\n",
            "    Wild Bunch, The (1969) (score=4.740)\n",
            "    My Life and Times With Antonin Artaud (En compagnie d'Antonin Artaud) (1993) (score=4.738)\n",
            "    Bullets Over Broadway (1994) (score=4.726)\n",
            "    Contempt (Mpris, Le) (1963) (score=4.715)\n",
            "    M (1931) (score=4.713)\n",
            " SVD-based:\n",
            "    2001: A Space Odyssey (1968) (score=4.843)\n",
            "    Leaving Las Vegas (1995) (score=4.601)\n",
            "    Terminator, The (1984) (score=4.456)\n",
            "    Clockwork Orange, A (1971) (score=4.445)\n",
            "    Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963) (score=4.404)\n",
            "    Mars Attacks! (1996) (score=4.398)\n",
            "    Citizen Kane (1941) (score=4.379)\n",
            "    Apocalypse Now (1979) (score=4.368)\n",
            "\n",
            "User 26:\n",
            " User-based:\n",
            "    Withnail and I (1987) (score=5.000)\n",
            "    Unbearable Lightness of Being, The (1988) (score=5.000)\n",
            "    Strawberry and Chocolate (Fresa y chocolate) (1993) (score=5.000)\n",
            "    Burnt By the Sun (1994) (score=5.000)\n",
            "    Double vie de Vronique, La (Double Life of Veronique, The) (1991) (score=5.000)\n",
            "    Tin Drum, The (Blechtrommel, Die) (1979) (score=5.000)\n",
            "    Three Colors: White (1994) (score=5.000)\n",
            "    Cook the Thief His Wife & Her Lover, The (1989) (score=5.000)\n",
            " Item-based:\n",
            "    Lashou shentan (1992) (score=4.471)\n",
            "    Yankee Zulu (1994) (score=4.471)\n",
            "    Symphonie pastorale, La (1946) (score=4.471)\n",
            "    Tigrero: A Film That Was Never Made (1994) (score=4.471)\n",
            "    To Cross the Rubicon (1991) (score=4.471)\n",
            "    Promise, The (Versprechen, Das) (1994) (score=4.471)\n",
            "    Eye of Vichy, The (Oeil de Vichy, L') (1993) (score=4.471)\n",
            "    Hostile Intentions (1994) (score=4.471)\n",
            " SVD-based:\n",
            "    Good Will Hunting (1997) (score=3.476)\n",
            "    Chasing Amy (1997) (score=3.394)\n",
            "    Empire Strikes Back, The (1980) (score=3.291)\n",
            "    E.T. the Extra-Terrestrial (1982) (score=3.280)\n",
            "    Secrets & Lies (1996) (score=3.248)\n",
            "    Boogie Nights (1997) (score=3.238)\n",
            "    Silence of the Lambs, The (1991) (score=3.212)\n",
            "    Lone Star (1996) (score=3.199)\n",
            "\n",
            "User 760:\n",
            " User-based:\n",
            "    Evita (1996) (score=5.000)\n",
            "    Sneakers (1992) (score=5.000)\n",
            "    Rosencrantz and Guildenstern Are Dead (1990) (score=5.000)\n",
            "    Beautiful Thing (1996) (score=5.000)\n",
            "    Picture Bride (1995) (score=5.000)\n",
            "    Sum of Us, The (1994) (score=5.000)\n",
            "    Jeffrey (1995) (score=5.000)\n",
            "    Field of Dreams (1989) (score=5.000)\n",
            " Item-based:\n",
            "    Entertaining Angels: The Dorothy Day Story (1996) (score=5.000)\n",
            "    For Love or Money (1993) (score=3.738)\n",
            "    Hugo Pool (1997) (score=3.709)\n",
            "    Coldblooded (1995) (score=3.697)\n",
            "    Across the Sea of Time (1995) (score=3.682)\n",
            "    War at Home, The (1996) (score=3.680)\n",
            "    Sunchaser, The (1996) (score=3.680)\n",
            "    Commandments (1997) (score=3.673)\n",
            " SVD-based:\n",
            "    Aladdin (1992) (score=3.432)\n",
            "    L.A. Confidential (1997) (score=3.396)\n",
            "    Sense and Sensibility (1995) (score=3.389)\n",
            "    Star Trek: First Contact (1996) (score=3.388)\n",
            "    Good Will Hunting (1997) (score=3.381)\n",
            "    Apt Pupil (1998) (score=3.371)\n",
            "    Terminator 2: Judgment Day (1991) (score=3.369)\n",
            "    Cold Comfort Farm (1995) (score=3.364)\n",
            "\n",
            "User 282:\n",
            " User-based:\n",
            "    Thousand Acres, A (1997) (score=5.000)\n",
            "    Spice World (1997) (score=5.000)\n",
            "    Hugo Pool (1997) (score=5.000)\n",
            "    Spitfire Grill, The (1996) (score=5.000)\n",
            "    Annie Hall (1977) (score=5.000)\n",
            "    Star Wars (1977) (score=5.000)\n",
            "    Godfather, The (1972) (score=5.000)\n",
            "    Chinatown (1974) (score=5.000)\n",
            " Item-based:\n",
            "    They Made Me a Criminal (1939) (score=5.000)\n",
            "    All Things Fair (1996) (score=5.000)\n",
            "    Jupiter's Wife (1994) (score=4.495)\n",
            "    Witness (1985) (score=4.478)\n",
            "    Vie est belle, La (Life is Rosey) (1987) (score=4.463)\n",
            "    Touki Bouki (Journey of the Hyena) (1973) (score=4.463)\n",
            "    Lashou shentan (1992) (score=4.463)\n",
            "    T-Men (1947) (score=4.463)\n",
            " SVD-based:\n",
            "    Titanic (1997) (score=3.757)\n",
            "    Star Wars (1977) (score=3.724)\n",
            "    Good Will Hunting (1997) (score=3.715)\n",
            "    Jerry Maguire (1996) (score=3.655)\n",
            "    Twelve Monkeys (1995) (score=3.650)\n",
            "    Apt Pupil (1998) (score=3.645)\n",
            "    First Wives Club, The (1996) (score=3.639)\n",
            "    Close Shave, A (1995) (score=3.625)\n",
            "\n",
            "All done  outputs are in the ./outputs folder. If Kaggle download failed earlier, accept the dataset license on Kaggle and re-run the cell.\n"
          ]
        }
      ]
    }
  ]
}